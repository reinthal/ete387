{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and train two neural language models: the fixed-window model and the recurrent neural network model. You will evaluate these models by computing their perplexity on a benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you should use a GPU if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # NVIDIA\n",
    "# device = torch.device('mps')    # Apple Silicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab is [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100 million tokens extracted from the ‚ÄúGood‚Äù and ‚ÄúFeatured‚Äù articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\n",
    "\n",
    "The next cell contains code for an object that will act as a container for the ‚Äútraining‚Äù and the ‚Äúvalidation‚Äù section of the data. We fill this container by reading the corresponding text files. The only processing we do is to split at whitespace and replace each newline with an end-of-sentence token. Importantly, we also build the vocabulary (`self.vocab`) that maps each word to an integer id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.train = self.read_data('wiki.train.tokens')\n",
    "        self.valid = self.read_data('wiki.valid.tokens')\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        ids = []\n",
    "        with open(path, encoding='utf-8') as source:\n",
    "            for line in source:\n",
    "                line = line.rstrip()\n",
    "                if line:\n",
    "                    for token in line.split() + ['<eos>']:\n",
    "                        if token not in self.vocab:\n",
    "                            self.vocab[token] = len(self.vocab)\n",
    "                        ids.append(self.vocab[token])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the data and prints the total number of tokens and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext = WikiText()\n",
    "\n",
    "print('Tokens in train:', len(wikitext.train))\n",
    "print('Tokens in valid:', len(wikitext.valid))\n",
    "print('Vocabulary size:', len(wikitext.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement and train the fixed-window neural language model proposed by [Bengio et al. (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and presented in the lectures. Recall that an input to this model takes the form of a vector of $n-1$ integer ids representing preceding words. We will refer to this vector as the *context window* and to its length as the *window size*. Each word id is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU). The output of that network can be interpreted as a categorical probability distribution over all possible words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1: Vectorise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement this as a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). Read the documentation of that class and complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    def __init__(self, word_ids, window_size):\n",
    "        self.token_ids = word_ids\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        raise NotImplemented\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should implement the following specification:\n",
    "\n",
    "**__init__** (*self*, *word_ids*, *window_size*)\n",
    "\n",
    "> Creates a new dataset, wrapping the underlying WikiText data *word_ids* (a list of word ids). The parameter *window_size* specifies the size of the context window.\n",
    "\n",
    "**__len__** (*self*)\n",
    "\n",
    "> Returns the number of samples in this dataset.\n",
    "\n",
    "**__getitem__** (*self*, *idx*)\n",
    "\n",
    "> Fetches a data sample for the given index (*idx*). A sample is a pair $(\\mathbf{x}, y)$ representing a context window and the next word after that window, where $\\mathbf{x}$ is a vector of length *window_size* and $y$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_11():\n",
    "    # Create the model-specific dataset\n",
    "    dataset = FixedWindowDataset(wikitext.valid, 2)\n",
    "\n",
    "    # Print the number of samples\n",
    "    print(len(dataset))\n",
    "\n",
    "    # Fetch a sample and print it\n",
    "    print(dataset[42])\n",
    "\n",
    "test_11()\n",
    "\n",
    "# Expected output:\n",
    "# 216345\n",
    "# tensor([22, 17]) tensor(1204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2: Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the fixed-window model based on the graphical specification given in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        # TODO: Add your own code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the two methods:\n",
    "\n",
    "**__init__** (*self*, *window_size*, *n_words*, *embedding_dim*=50, *hidden_dim*=50)\n",
    "\n",
    "> Creates a new fixed-window neural language model. The argument *window_size* specifies the length of the context window. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the output dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, s)$, where $B$ is the batch size and $s$ is the window size. The output of the forward pass is a tensor of shape $(B, V)$ where $V$ is the number of words in the vocabulary.\n",
    "\n",
    "#### ü§û Test your code\n",
    "\n",
    "The following code instantiates the model and feeds it batches of samples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_12():\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Set the context window size\n",
    "    window_size = 2\n",
    "\n",
    "    # Instantiate a small dataset and a data loader\n",
    "    dataset = FixedWindowDataset(wikitext.train[:30], window_size)\n",
    "    data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = FixedWindowModel(window_size, len(wikitext.vocab))\n",
    "\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        # Feed the model a batch of samples from the training data\n",
    "        output = model(batch_x)\n",
    "\n",
    "        # Print the shape of the model output\n",
    "        print(output.shape)\n",
    "\n",
    "test_12()\n",
    "\n",
    "# Expected output:\n",
    "# torch.Size([10, 33278])\n",
    "# torch.Size([10, 33278])\n",
    "# torch.Size([8, 33278])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Questions for the oral report\n",
    "\n",
    "* What do the numbers 30 and 10 refer to in the test code?\n",
    "* How do these numbers affect the output shapes?\n",
    "* What parameters in the model affect the output shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final task is to write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops you have already seen. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window(window_size, n_epochs=1, batch_size=512, lr=1e-3):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*window_size*, *n_epochs* = 1, *batch_size* = 512, *lr* = 0.001)\n",
    "\n",
    "> Trains a fixed-window neural language model with context window size *window_size* using minibatch gradient descent and returns the trained model. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below trains a model with window size 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed_window = train_fixed_window(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n",
    "\n",
    "* Computing the validation perplexity in one go (for the full validation set) will most probably exhaust your computer‚Äôs memory and/or take a lot of time. If you run into this problem, do the computation at the minibatch level and aggregate the results.\n",
    "* Training and even evaluation will take some time ‚Äì when using a CPU, you should expect several minutes per epoch, depending on the hardware. Our reference implementation uses a GPU and runs in less than 30 seconds per epoch on [Colab](http://colab.research.google.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 400 after the first epoch.** You should not change the parameters of the model or the training to meet this target.\n",
    "\n",
    "To see whether your network is learning something, print or plot the running loss on the training data. If this value not decrease during training, try to find the problem before wasting time (and electricity) on useless computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Recurrent neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each id is mapped to an embedding vector, and the sequence of these vectors is then fed into an unrolled LSTM. At each position $i$ in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation whose output can be interpreted as a categorical probability distribution over the words at position $i+1$. In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Vectorise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model. The *input sequences* in this model-specific dataset are obtained by partitioning the data into non-overlapping segments. The *output sequences* correspond to the input sequences shifted one position to the right, so that corresponding elements in the two sequences represent words and next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RNNDataset(Dataset):\n",
    "    def __init__(self, word_ids, seq_len):\n",
    "        self.token_ids = word_ids\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        raise NotImplemented\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should implement the following specification:\n",
    "\n",
    "**__init__** (*self*, *word_ids*, *seq_len*)\n",
    "\n",
    "> Creates a new dataset, wrapping the underlying WikiText data *word_ids* (a list of word ids). The parameter *seq_len* specifies the length of an input sequence.\n",
    "\n",
    "**__len__** (*self*)\n",
    "\n",
    "> Returns the number of samples in this dataset.\n",
    "\n",
    "**__getitem__** (*self*, *idx*)\n",
    "\n",
    "> Fetches a data sample for the given index (*idx*). A sample is a pair $(\\mathbf{x}, \\mathbf{y})$ representing contiguous subsequences of the underlying data. Compared to the input sequence, the output sequence is shifted one position to the right. More precisely, if $\\mathbf{x}$ is the sequence that starts at token position $k$, then $\\mathbf{y}$ is the sequence that starts at position $k+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_21():\n",
    "    # Create the model-specific dataset\n",
    "    dataset = RNNDataset(wikitext.valid, 5)\n",
    "\n",
    "    # Print the number of samples\n",
    "    print(len(dataset))\n",
    "\n",
    "    # Fetch a sample and print it\n",
    "    print(dataset[42])\n",
    "\n",
    "test_21()\n",
    "\n",
    "# Expected output:\n",
    "# 43269\n",
    "# (tensor([1179, 2376, 1839, 1450, 1179]), tensor([2376, 1839, 1450, 1179, 1450]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2: Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the recurrent neural network model based on the graphical specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        # TODO: Add your own code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should follow this specification:\n",
    "\n",
    "**__init__** (*self*, *n_words*, *embedding_dim* = 50, *hidden_dim* = 50)\n",
    "\n",
    "> Creates a new recurrent neural network language model based on an LSTM. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, H)$, where $B$ is the batch size and $H$ is the length of each input sequence. The shape of the output tensor is $(B, H, V)$, where $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_22():\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Set the sequence length\n",
    "    seq_len = 5\n",
    "\n",
    "    # Instantiate a small dataset and a data loader\n",
    "    dataset = RNNDataset(wikitext.train[:51], seq_len)\n",
    "    data_loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = RNNModel(len(wikitext.vocab))\n",
    "\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        # Feed the model a batch of samples from the training data\n",
    "        output = model(batch_x)\n",
    "\n",
    "        # Print the shape of the model output\n",
    "        print(output.shape)\n",
    "\n",
    "test_22()\n",
    "\n",
    "# Expected output:\n",
    "# torch.Size([3, 5, 33278])\n",
    "# torch.Size([3, 5, 33278])\n",
    "# torch.Size([3, 5, 33278])\n",
    "# torch.Size([1, 5, 33278])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Questions for the oral report\n",
    "\n",
    "* What do the numbers 5, 51 and 3 refer to in the test code?\n",
    "* How do these numbers affect the output shapes?\n",
    "* What is the total number of samples in the small subset of the data used in the test code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(seq_len=32, n_epochs=1, batch_size=16, lr=1e-2):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_rnn** (*seq_len* = 32, *n_epochs* = 1, *batch_size* = 16, *lr* = 0.01)\n",
    "\n",
    "> Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameter *seq_len* specifies the length of the input and output sequences. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = train_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 280 after the first epoch.** You should not have to change the parameters of the model or the training to meet this target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Parameter initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose ‚Äúgood‚Äù initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution $\\mathcal{N}(0, 1)$. How do different initialisations affect the perplexity of your fixed-window language model?\n",
    "\n",
    "Run a small experiment where you try a few non-standard normal distributions with different values for the mean and the variance. Document your results in a table and add it to this notebook. Prepare the following questions for the oral report:\n",
    "\n",
    "* What different parameter combinations did you try? What results did you get?\n",
    "* How do you interpret your results? Did they match your expectations?\n",
    "* What did you learn? Why does this learning matter?\n",
    "\n",
    "Several authors have developed a theory for more principled choices of initialisation strategies based on properties of the specific network. Two standard articles in this area are [Glorot and Bengio (2010)](https://proceedings.mlr.press/v9/glorot10a.html) and [He et al. (2015)](https://doi.org/10.1109/ICCV.2015.123), whose initialisation strategies have been implemented in the [`nn.init`](https://pytorch.org/docs/stable/nn.init.html) module. Have a look at the articles and the PyTorch documentation if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing Lab 1!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
