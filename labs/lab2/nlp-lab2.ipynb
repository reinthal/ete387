{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will dive into the inner workings of the GPT architecture. You will walk through a complete implementation of the architecture in PyTorch, instantiate this implementation with pre-trained weights, and put the resulting model to the test by generating text. At the end of this lab, you will understand the building blocks of the GPT architecture and how they are connected.\n",
    "\n",
    "*Tasks you can choose for the oral exam are marked with the graduation cap ðŸŽ“ emoji.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPT architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 was first described by [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). To faithfully implement the model, one needs to also read the earlier paper by [Radford et al. (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Another important source of information is the code released by OpenAI, which is available on GitHub ([link](https://github.com/openai/gpt-2)).\n",
    "\n",
    "The GPT architecture is made up of a stack of Transformer blocks. Each block has two main parts: one handles multi-head self-attention, and the other is a feed-forward network. Before these parts do their work, their input undergoes layer normalisation, and residual connections are added to help the model learn more effectively. The input to the architecture is a sequence of token IDs; these are turned into embeddings and augmented with information about the absolute position of each token in the sequence. The output layer converts the internal representations into logit scores for every token in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) present four increasingly larger GPT models based on the same architecture. Here, we will implement the smallest of these, characterised by the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    n_vocab: int = 50_257\n",
    "    n_ctx: int = 1024\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 12\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 2.01: Model configuration\n",
    "\n",
    "Explain the purpose of these hyperparameters. In particular, where does the number 50,257 come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_vocab` This is the size of the vocabulary. That is, the number of unique tokens the model can interpret. The number comes from expanding the initial 256 unique unicode 50000 times plus one token for end-of-text. They used the greedy BPE algorithm. It doesnt have the UNK token due to BPE managing to encode everything.\n",
    "- `n_ctr` How many tokens can be considered at any one time?\n",
    "- `n_embd` the embedding dimensions of each token\n",
    "- `n_head` Number of self-attention heads\n",
    "- `n_layer` is the number of neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GELU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the feed-forward network. This is a standard two-layer network with a Gaussian Error Linear Unit (GELU) activation function ([Hendrycks and Gimpel, 2016](https://doi.org/10.48550/arXiv.1606.08415)).\n",
    "\n",
    "The GELU is a smooth version of the rectified linear unit (ReLU) that weights inputs by their value under the cumulative distribution function of the standard Gaussian. This function is commonly denoted by $\\Phi$. For example, $\\text{GELU}(0{.}5) = 0{.}5 \\cdot \\Phi(0{.}5) \\approx 0{.}5 \\cdot 0{.}6915 = 0{.}3457$ because approximately 69.15% of normally distributed data lies to the left of $0{.}5$.\n",
    "\n",
    "When GPT-2 was released, computing the GELU exactly was expensive, and the released code therefore used an approximation originally presented by [Page (1977)](https://doi.org/10.2307/2346872). We follow suit here, as we want to create a replica of the original model. However, it is worth mentioning that PyTorch now offers an exact implementation of the GELU so fast that using an approximation is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh((2 / torch.pi) ** 0.5 * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.02: Mathematical properties of the GELU\n",
    "\n",
    "Find the minimal output value of the (approximated) GELU and the input value for which it yields that output. Use a service such as [WolframAlpha](https://www.wolframalpha.com/) for the necessary derivations. What are the main differences between the GELU and the ReLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\min(\\textrm{gelu}(x)) = -0.1700, x \\approx -0.75$\n",
    "\n",
    "Main differences are\n",
    "- That ReLu is never negative.\n",
    "- Gelu is smooth and differentiable everywhere, in particular at $x=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here is the code for the feed-forward network. Note that we follow the released code and use the name **multi-layer perceptron (MLP)** rather than â€œfeed-forward networkâ€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        batch_size, seq_len, n_embd = x.shape\n",
    "        x = self.c_fc(x) # shape: [batch_size, seq_len, n_embd * 4]\n",
    "        x = gelu(x) # shape: [batch_size, seq_len, n_embd * 4]\n",
    "        x = self.c_proj(x) # shape: [batch_size, seq_len, n_embd]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.03: Shape annotations\n",
    "\n",
    "One of the most common errors in deep learning is a mismatch in tensor dimensions. To avoid this, it is good practice to annotate PyTorch code with shapes. For example, suppose you are given the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Linear(5, 7)\n",
    "x = torch.rand(2, 3, 5)\n",
    "y = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotation of this code with shapes would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Linear(5, 7)\n",
    "# not a tensor variable; needs no annotation\n",
    "\n",
    "x = torch.rand(2, 3, 5)\n",
    "# shape of x: [2, 3, 5]\n",
    "\n",
    "y = f(x)\n",
    "# shape of y: [2, 3, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the shapes in the `forward()` method of the feed-forward network. Instead of using actual numbers, refer to dimension sizes by symbolic names such as `n_embd`, `batch_size` (number of samples in a batch of input data) and `seq_len` (length of an input sequence). You can introduce additional names and other notation you find useful. Make your annotations as detailed as you need them to explain how the shapes change from one line to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal is to implement the core of the GPT architecture: the multi-head attention mechanism.\n",
    "\n",
    "Recall that the attention mechanism in the Transformer decoder must be restricted to attending only to previously generated tokens. This type of attention is also called **causal attention**. In practice, we implement it through a masking technique that sets the post-softmax attention weights of future tokens to zero. The following utility function implements such a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(n):\n",
    "    return torch.triu(torch.full((n, n), float(\"-inf\")), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 2.04: Causal mask\n",
    "\n",
    "Have a close look at the following code and run it to see the result. What are the shapes of `x` and `mask`? Given that the shapes are different, why does the addition operation in the last line not raise an error? What is the shape of the result?\n",
    "\n",
    "How does the addition operation implement masking? (Recall that the attention scores are normalised using the softmax function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 3, 3)\n",
    "mask = make_causal_mask(5) # mask: [5, 5]\n",
    "\n",
    "res = x + mask[:3, :3] # x: [1, 2, 3, 3] + [3, 3] => broadcasting over dims [1, 2] = [1, 2, 3, 3]\n",
    "print(res.shape) # res: [1, 2, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't cause an error because PyTorch uses something called broadcasting which adds the matrix `mask[3,3]` to all other matrices in `x`. To broadcast, PyTorch looks for matching dimensions moving backwards in the tensor dimensions, from 3 to 3,3 to 3,3,2 etc. In our example we see that the dimenions of the mask $3 \\times 3$ matches with the last two dimensions of the shape of $x$. \n",
    "\n",
    "How does the mask stay upper triangular? Because any submatrix of mask will also be upper triangular if you move left to right. See matrix below: \n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\color{cyan}0 & \\color{cyan}-\\infty & \\color{cyan}-\\infty & -\\infty & -\\infty \\\\\n",
    "\\color{cyan}0 & \\color{cyan}0 & \\color{cyan}-\\infty & -\\infty & -\\infty \\\\\n",
    "\\color{cyan}0 & \\color{cyan}0 & \\color{cyan}0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for the multi-head attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.register_buffer(\"mask\", make_causal_mask(config.n_ctx), persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.shape\n",
    "        head_embd = n_embd // self.n_head # shape: 1, scalar broadcasting\n",
    "        # Technically broadcasting also for layer computations\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1) # We use same dims for q, k, v => we use the same Linear layer for computing them along the last dimension of Liner layer\n",
    "        q = q.view(batch_size, seq_len, self.n_head, head_embd) # shape: (batch_size, seq_len, n_head, head_embd)\n",
    "        k = k.view(batch_size, seq_len, self.n_head, head_embd) # shape: (batch_size, seq_len, n_head, head_embd)\n",
    "        v = v.view(batch_size, seq_len, self.n_head, head_embd) # shape: (batch_size, seq_len, n_head, head_embd)\n",
    "        q = q.transpose(-2, -3) # shape: (batch_size, n_head, seq_len, head_embd)\n",
    "        k = k.transpose(-2, -3) # shape: (batch_size, n_head, seq_len, head_embd)\n",
    "        v = v.transpose(-2, -3) # shape: (batch_size, n_head, seq_len, head_embd)\n",
    "        x = q @ k.transpose(-1, -2) # shape: (batch_size, n_head, seq_len, seq_len)\n",
    "        x = x / head_embd**0.5 # normalize x by root n scalar, same dims\n",
    "        x = x + self.mask[:seq_len, :seq_len] # shape: (batch_size, n_head, seq_len, seq_len) Broadcast\n",
    "        x = torch.softmax(x, dim=-1) # shape: (batch_size, n_head, seq_len, seq_len)\n",
    "        x = x @ v # shape: (batch_size, n_head, seq_len, head_embd)\n",
    "        x = x.transpose(-2, -3).contiguous()  # shape: (batch_size, seq_len, n_head, head_embd), makes x contiguous in memory\n",
    "        x = x.view(batch_size, seq_len, n_embd) # shape: (batch_size, seq_len, n_embd), \n",
    "        # Technically broadcasting also for layer computations\n",
    "        x = self.c_proj(x) # shape: (batch_size, seq_len, n_embd)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.05: Multi-head attention\n",
    "\n",
    "Trace the input `x` through the `forward()` method line by line and annotate the shapes of all tensor variables. Identify all lines that rely on broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the inputs to both the feed-forward network and the multi-head attention mechanism undergo **layer normalisation**. This normalises the inputs to have zero mean and unit variance across the activations. [Ba et al. (2016)](https://doi.org/10.48550/arXiv.1607.06450) introduce two trainable parameters (called $\\gamma$ and $\\beta$ in the paper) that allow the network to learn an appropriate scale and shift for the normalised values.\n",
    "\n",
    "We implement layer normalisation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(config.n_embd))\n",
    "        self.b = nn.Parameter(torch.zeros(config.n_embd))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(unbiased=False, dim=-1, keepdim=True)\n",
    "        return self.g * (x - mean) / torch.sqrt(variance + 1e-05) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽˆ Task 2.06: Layer normalisation\n",
    "\n",
    "What is the relevance of the `keepdim=True` keyword argument in the `mean()` and `var()` functions? What would happen if we omitted it?\n",
    "\n",
    "What is the relevance of the constant 1e-05? What could happen if we omitted it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keepdim=true` makes sure we calculate the statistics along the correct dimension. In this case, we calculate it along the last dimension which is typically the dimension reserved for logits.\n",
    "\n",
    "From the docs\n",
    "\n",
    "```python\n",
    "If :attr:`keepdim` is ``True``, the output tensor is of the same size\n",
    "as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\n",
    "Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\n",
    "output tensor having 1 (or ``len(dim)``) fewer dimension(s).\n",
    "```\n",
    "\n",
    "the `1e-05` is for making sure we do not divide by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine the feed-forward network, the multi-head attention mechanism and the layer normalisation into a decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln_2 = LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.07: Pre-norm and post-norm architectures\n",
    "\n",
    "The original Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) is a â€œpost-norm architectureâ€, where the normalisation is applied **after** each residual block. In contrast, GPT-2 is a â€œpre-norm architectureâ€, where the normalisation is applied **before**. Find the passage in Section&nbsp;2.3 of [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that reports on this modification.\n",
    "\n",
    "[Xiong et al. (2020)](https://arxiv.org/pdf/2002.04745) compare pre-norm and post-norm architectures empirically. Read the abstract of their paper and summarise their main findings. According to these findings, what are the benefits of the pre-norm architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They found a theoretical result that shows that post-LN causes exploding gradients and unstable training which has motivated warm up learning rate\n",
    "- They found another theoretical result that shows that pre-LN gradients are well behaved at initialization of the network\n",
    "- They show empirically that they can train networks faster without warmup and likely less compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have almost all components in place to complete the implementation of the GPT-2 model. The only thing  missing are the position embeddings. These simply associate an embedding vector with every position in the context window. To set them up, we first define another utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positions(n):\n",
    "    return torch.arange(n, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then code the complete model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.n_vocab, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_ctx, config.n_embd)\n",
    "        self.h = nn.Sequential(*(Block(config) for _ in range(config.n_layer)))\n",
    "        self.ln_f = LayerNorm(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.n_vocab, bias=False)\n",
    "        self.register_buffer(\"pos\", make_positions(config.n_ctx), persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        wte = self.wte(x)\n",
    "        wpe = self.wpe(self.pos[:seq_len]) # type: ignore\n",
    "        x = wte + wpe\n",
    "        x = self.h(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.09: Number of trainable parameters\n",
    "\n",
    "The model we have implemented is the smallest one presented by [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). But how many trainable parameters exactly does it have? Interestingly, the number originally reported by the authors is wrong! What number did they report?\n",
    "\n",
    "Your task is to write code to compute the number of parameters yourself. This should only take 1â€“3 lines of code. What number do you get when you apply this code to a fresh model instance?\n",
    "\n",
    "[Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) followed the original Transformers paper ([Vaswani et al., 2017](https://doi.org/10.48550/arXiv.1706.03762)) and shared the trainable weights between the token embedding and the final linear layer. Implement this weight sharing strategy. (Hint: This only requires one line of code.) Then, re-compute the number of trainable parameters for the modified model. What number do you get now? How large is the reduction caused by the weight sharing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a complete implementation of the GPT-2 model in place, you can instantiate it by loading the pre-trained weights released by OpenAI. These weights were originally provided in the TensorFlow format. For this lab, we have re-packaged them as a single file in NumPyâ€™s `.npz` archive format. We can load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pretrained = np.load(\"gpt-2-pretrained.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result `pretrained` is a dictionary mapping names to NumPy arrays. When you print the names, you will see that they correspond to the attributes of our network modules, even though the names differ. For example, the array `h0.attn.c_attn.b` holds the biases (`b`) of the `c_attn` linear layer in the attention mechanism (`attn`) of the first transformer block (`h0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.10: Load pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model from the pre-trained weights. To do this, you need to instantiate a fresh model and write the contents of each array from the `npz` archive with the pre-trained weights into the corresponding tensor. To make this a bit easier, here is a utility function that copies data from a NumPy array `source` to a PyTorch tensor `target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(source: np.ndarray, target: torch.Tensor):\n",
    "    assert source.shape == target.shape\n",
    "    with torch.no_grad():\n",
    "        target.copy_(torch.tensor(source, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start from this skeleton code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pretrained() -> Model:\n",
    "    model = Model(Config())\n",
    "    pretrained = np.load(\"gpt-2-pretrained.npz\")\n",
    "    # TODO: Copy the weights from `pretrained` to `model`\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** One technical detail to note is that PyTorch stores the weights of linear layers in a transposed form. For example, a linear layer created as `nn.Linear(2, 3)` has a weight matrix of shape [3, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Put the model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third and final part of this lab, you will use the pre-trained model to generate text and evaluate it on a standard benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling-based text generation\n",
    "\n",
    "The easiest way to generate text with a language model is by using a **greedy approach**. This method works by creating text one token at a time. At each step, the model takes the previously generated text (called the **context**) as input and adds the token with the highest output logit as a new token. The code in the next cell defines a function `generate()` that forms the core of a greedy generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, context, context_size=1024, n_tokens=20):\n",
    "    for _ in range(n_tokens):\n",
    "        context = context[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)[:, -1, :]\n",
    "        next_idx = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        context = torch.cat([context, next_idx], dim=-1)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function with an actual text input, you need a tokeniser to first encode the text into a vector of token IDs, and later decode the generated `context` into new text. The reference implementation of the GPT-2 tokeniser is in the library `tiktoken`. The code in the next cell sets up the tokeniser, loads the pretrained model from Task&nbsp;2.10, and then defines a helper function that handles the encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = from_pretrained()\n",
    "\n",
    "\n",
    "def generate_helper(text, context_size=1024, n_tokens=20):\n",
    "    context = torch.tensor([tokenizer.encode(text)], dtype=torch.long)\n",
    "    context = generate(model, context, context_size=context_size, n_tokens=n_tokens)\n",
    "    return tokenizer.decode(context[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this helper function to generate text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_helper(\"LinkÃ¶ping University is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** If you did not manage to complete Task&nbsp;2.10, you can still work on this task by using a pretrained GPT-2 model from [Hugging Face](https://huggingface.co/openai-community/gpt2). The next code cell shows how you would instantiate this model. Note that you may have to first install the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# logits = model(context).logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.11: Sampling-based text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The greedy approach to text generation is not very interesting for practical applications because it always chooses the most likely token, leading to predictable and less creative results. Your task is to modify the code for the `generate()` function to use a **sampling-based approach** instead. In this approach, the next token is chosen randomly based on the probabilities assigned by the model (softmax-normalised logits), treating them as a categorical distribution over the token vocabulary. Additionally, your code should include two common techniques to improve sampling:\n",
    " \n",
    " * **temperature scaling**, which lets the user control the randomness of the sampling\n",
    " * **top-$k$ sampling**, which limits the sampling to the top-$k$ most likely tokens, ignoring less probable ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have experimented with your pretrained GPT-2 model, you will have noticed that its ability to generate useful text is somewhat limited. By todayâ€™s standards, GPT-2 is a small model with modest capabilities. However, it can still be helpful for certain tasks, such as text autocompletion, generating filler text, or answering simple questions. To rigourosly evaluate language models, researchers often use standard benchmark datasets. Creating these benchmarks is a discipline of its own, and they tend to become increasingly challenging as models continue to improve.\n",
    "\n",
    "In the final task of this lab, you will evaluate GPT-2â€™s performance on a small subset of the [HellaSwag dataset](https://rowanzellers.com/hellaswag/), which was published in the same year as GPT-2 itself (2019). HellaSwag is designed to test a modelâ€™s ability to perform commonsense reasoning in challenging contexts. Unlike simpler benchmarks, HellaSwag presents scenarios where the correct text completion depends on semantic relationships between events and on world knowledge. This makes it a good choice for assessing the ability of language models to go beyond surface-level patterns and produce meaningful, context-aware predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ“ Task 2.12: Evaluating the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the [HellaSwag website](https://rowanzellers.com/hellaswag/) to get some background on the benchmark. How does a sample from the dataset look like? What is an expected prediction? How does the benchmark allow us to score models? What is the random baseline? What is the human performance reported on the task?\n",
    "\n",
    "The next cell contains code for evaluating your pretrained model on a small sample from HellaSwag. You will also need a tokenizer. The HellaSwag subset is in the file `hellaswag-mini.jsonl`. Inspect that file to understand the format. Next, read the code and explain how it works. Specifically, how does the code compute the score of individual endings? In the call to `cross_entropy()`, why are the tensors sliced in this specific way?\n",
    "\n",
    "Finally, what overall score does the pretrained GPT-2 model get on this benchmark? How does that score compare to the random baseline and the human performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"hellaswag-mini.jsonl\") as f:\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        prefix = tokenizer.encode(sample[\"ctx\"])\n",
    "        ending_scores = []\n",
    "        for i, ending in enumerate(sample[\"endings\"]):\n",
    "            suffix = tokenizer.encode(\" \" + ending)\n",
    "            context = torch.tensor([prefix + suffix], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                logits = model(context)\n",
    "                ending_score = torch.nn.functional.cross_entropy(\n",
    "                    logits[0, -len(suffix) - 1 : -1], context[0, -len(suffix) :]\n",
    "                )\n",
    "            ending_scores.append((ending_score, i))\n",
    "        predicted = min(ending_scores)[1]\n",
    "        n_correct += int(predicted == sample[\"label\"])\n",
    "        n_total += 1\n",
    "    print(f\"Accuracy: {n_correct / n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ¥³ Congratulations on finishing lab&nbsp;2!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
